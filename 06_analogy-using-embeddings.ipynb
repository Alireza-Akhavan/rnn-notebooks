{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div style=\"direction:rtl;font-family:B Lotus, B Nazanin, Tahoma\">به نام خدا</div></center>\n",
    "<img src=\"./logo.png\" alt=\"class.vision\" style=\"width: 200px;\"/>\n",
    "<h1><center><div style=\"direction:rtl;font-family:B Lotus, B Nazanin, Tahoma\">قیاس کلمات (Word analogies)</div></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n",
    "کدها  با تغییرات برگرفته از کورس Sequence Models پروفسور Andrew NG است.\n",
    "</div>\n",
    "\n",
    "[https://www.coursera.org/learn/nlp-sequence-models](https://www.coursera.org/learn/nlp-sequence-models)\n",
    "\n",
    "<div style=\"direction:rtl;text-align:right;font-family:Tahoma\">\n",
    "بردار از قبل آموزش داده شده را می‌توانید از اینجا دانلود کنید:</div>\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = './temp'\n",
    "embedding_index = {}\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt'), 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Cosine similarity\n",
    "\n",
    "To measure how similar two words are, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n",
    "\n",
    "### Definition\n",
    "The cosine of two non-zero vectors can be derived by using the Euclidean dot product formula:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    A \\cdot B = \\lVert A \\rVert \\lVert B \\rVert cos(\\theta)\n",
    "\\end{equation}\n",
    "$$\n",
    "Given two $n$-dimensional vectors of attributes, $\\mathbf{A}$ and $\\mathbf{B}$, the cosine similarity, $cos(\\theta)$, is represented using a dot product and magnitude as:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{Cosine Similarity} = S_C(A, B) &= cos(\\theta)\\\\\n",
    "    &= \\frac{\\mathbf{A \\cdot B}}{\\lVert \\mathbf{A} \\rVert \\lVert \\mathbf{B} \\rVert}\\\\\n",
    "    &= \\frac{\\sum_{i=1}^{n}A_iB_i}{\\sqrt{\\sum_{i=1}^{n}A_i^2} \\cdot \\sqrt{\\sum_{i = 1}^{n}B_i^2}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(u, v):\n",
    "    dot = np.dot(u, v.T)\n",
    "    length_prod = np.sqrt(np.sum(np.square(u), axis=-1)) * np.sqrt(np.sum(np.square(v), axis=-1))\n",
    "    return dot / (length_prod + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(father, mother) =  0.8656660919003184\n",
      "cosine_similarity(ball, crocodile) =  0.1520657243738079\n",
      "cosine_similarity(france - paris, tehran - iran) =  -0.6854124336246552\n"
     ]
    }
   ],
   "source": [
    "father = embedding_index[\"father\"]\n",
    "mother = embedding_index[\"mother\"]\n",
    "ball = embedding_index[\"ball\"]\n",
    "crocodile = embedding_index[\"crocodile\"]\n",
    "france = embedding_index[\"france\"]\n",
    "tehran = embedding_index[\"tehran\"]\n",
    "paris = embedding_index[\"paris\"]\n",
    "iran = embedding_index[\"iran\"]\n",
    "print(\"cosine_similarity(father, mother) = \", similarity(father, mother))\n",
    "print(\"cosine_similarity(ball, crocodile) = \",similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(france - paris, tehran - iran) = \",similarity(france - paris, tehran - iran))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Word analogy task\n",
    "\n",
    "In the word analogy task, we complete the sentence <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. An example is <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. In detail, we are trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \\approx e_d - e_c$. We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64706 , -0.068067,  0.15468 , -0.17408 , -0.29134 ,  0.76999 ,\n",
       "       -0.3192  , -0.25663 , -0.25082 , -0.036737, -0.25509 ,  0.29636 ,\n",
       "        0.5776  ,  0.49641 ,  0.19167 , -0.83888 ,  0.58482 , -0.38717 ,\n",
       "       -0.71591 ,  0.9519  , -0.37966 , -0.1131  ,  0.47154 ,  0.20921 ,\n",
       "        0.38197 ,  0.067582, -0.92879 , -1.1237  ,  0.84831 ,  0.68744 ,\n",
       "       -0.15472 ,  0.92714 ,  0.53371 , -0.037392, -0.856   ,  0.19056 ,\n",
       "       -0.014594,  0.15186 ,  0.53514 , -0.20306 , -0.35164 ,  0.33152 ,\n",
       "        1.1306  , -0.72787 , -0.19724 ,  0.031659, -0.24041 , -0.057617,\n",
       "        0.60473 , -0.49233 , -0.24405 , -0.3184  ,  0.96156 ,  1.0895  ,\n",
       "        0.21534 , -2.0542  , -1.0615  ,  0.052439,  0.57958 ,  0.2748  ,\n",
       "        0.91587 ,  0.85195 ,  0.36113 , -0.31901 ,  0.7784  , -0.36865 ,\n",
       "        0.64387 ,  0.33104 , -0.27181 ,  0.58524 , -0.15143 ,  0.11121 ,\n",
       "        0.2126  , -0.60345 ,  0.16148 ,  0.32952 , -0.1354  , -0.30629 ,\n",
       "       -0.89143 ,  0.091912,  0.49753 ,  0.55932 ,  0.19329 ,  0.044859,\n",
       "       -1.0416  , -0.41566 , -0.54174 , -0.7244  , -0.57492 , -1.1188  ,\n",
       "        0.087097, -0.2992  ,  0.87227 ,  0.86996 , -0.89641 , -0.28259 ,\n",
       "       -0.47295 , -0.74062 , -0.39    , -0.78099 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index[\"father\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(word_a, word_b, word_c, embedding_index):\n",
    "    # convert words to lower case\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    # Get the word embeddings v_a, v_b and v_c \n",
    "    e_a, e_b, e_c = embedding_index[word_a], embedding_index[word_b], embedding_index[word_c]\n",
    "    # Get all the words in an array\n",
    "    words = np.array(list(embedding_index.keys()))\n",
    "\n",
    "    # Get index of each word\n",
    "    ## This is done so to avoid word_d being the same thing\n",
    "    index_a = np.argmax(words == word_a)\n",
    "    index_b = np.argmax(words == word_b)\n",
    "    index_c = np.argmax(words == word_c)\n",
    "    \n",
    "    embedding_matrix = np.array(list(embedding_index.values()), dtype='float32')\n",
    "    # Calculate e_b - e_a\n",
    "    part_1 = e_b - e_a\n",
    "    # Calculate every word as e_d - e_c\n",
    "    part_2 = embedding_matrix - e_c\n",
    "    # Calculate similarities for each word (return (num_words, 1))\n",
    "    similarities = similarity(part_2, part_1)\n",
    "    # avoid word_d to be the same word as word_a, word_b and word_c\n",
    "    similarities[[index_a, index_b, index_c]] = -100\n",
    "    # select the most similar word\n",
    "    selected_word = words[np.argmax(similarities)]\n",
    "    return selected_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iranian'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('china', 'chinese', 'iran', embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tehran'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('india', 'delhi', 'iran', embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'girl'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('man', 'woman', 'boy', embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bigger'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('small', 'smaller', 'big', embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inuktitut'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('iran', 'farsi', 'canada', embedding_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<div style=\"direction:rtl;text-align:right;font-family:B Lotus, B Nazanin, Tahoma\"> دوره پیشرفته یادگیری عمیق<br>علیرضا اخوان پور<br>  آبان و آذر 1399<br>\n",
    "</div>\n",
    "<a href=\"http://class.vision\">Class.Vision</a> - <a href=\"http://AkhavanPour.ir\">AkhavanPour.ir</a> - <a href=\"https://github.com/Alireza-Akhavan/\">GitHub</a>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "8hb5s",
   "launcher_item_id": "5NrJ6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
